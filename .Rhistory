text=element_text(size=18))
ggplot(autocorrelation_plot_frame,aes(x=t,y=walk))+
geom_point()+
autocorrelation_theme
## First step is to generate a random walk
nsteps<-1000 # Set number of steps in random walk
set.seed(75197) # Initializing a random seed so results are reproducible
random_walk<-cumsum(rnorm(nsteps)) # Moving randomly on a 1D axis at each step according to SN
inter_step_diffs<-random_walk-dplyr::lag(random_walk,1)
inter_step_diffs[1]<-0
previous_t<-dplyr::lag(random_walk,1)
autocorrelation_plot_frame<-data.frame(t=1:nsteps,
walk=random_walk,
diffs=inter_step_diffs,
prev_step=previous_t)
autocorrelation_theme<-theme(panel.background=element_blank(),
text=element_text(size=18))
ggplot(autocorrelation_plot_frame,aes(x=t,y=walk))+
geom_point()+
autocorrelation_theme
linear_model<-lm(data=autocorrelation_plot_frame,
walk~prev_step)
summary(linear_model)
summary(linear_model)
residual_model<-lm(data=autocorrelation_plot_frame,
walk~diffs)
summary(residual_model)
residual_model<-lm(data=autocorrelation_plot_frame,
diffs~t)
summary(residual_model)
ggplot(autocorrelation_plot_frame,aes(x=t,y=diffs))+
geom_point()+
autocorrelation_theme+
ylab('X(t)-X(t-1)')+
xlab('t')+
ggtitle('Residuals of Random Walk (R^2~0)')
library(tidyverse)
# Make formula for "true curve"
# Say y=2x^2+0.5x-1 sure
x_coords<-1:4
y_coords<-2*x_coords^2+(x_coords)/2-1
x_coords
y_coords
y_coords<-(x_coords^2)/2+2*(x_coords)/2-1
y_coords<-(x_coords^2)/2+2*(x_coords)-1
y_coords
plot(x_coords,y_coords)
m_estimate<-(y_coords[2]-y_coords[1])/(x_coords[2]-x_coords[1])
b_estimate<-y_coords[1]-m_estimate*x_coords[1]
m_estimate
b_estimate
plot(obs_set_x,obs_set_y)
# Now say we were to try to fit a linear model to these data.
#  A linear model in this case will be defined as:
# y=mx+b
# X and Y are our data, so there are two parameters to estimate. If we have 2 points, we can estimate both of them
obs_set_x<-x_coords[1:2]
plot(obs_set_x,obs_set_x)
plot(obs_set_x,obs_set_y)
obs_set_y<-y_coords[1:2]
plot(obs_set_x,obs_set_y)
lines(obs_set_x,m_estimate*obs_set_x+b_estimate)
rhs_a<-obs_set_x[2]^2-obs_set_x[1]^2
## First step is to generate a random walk
nsteps<-1000 # Set number of steps in random walk
library(tidyverse)
tibble
library(tibble)
unloadNamespace(tidyverse)
unloadNamespace('tidyverse')
library(tidyverse)
#library(tidyverse)
## First step is to generate a random walk
nsteps<-1000 # Set number of steps in random walk
set.seed(75197) # Initializing a random seed so results are reproducible
random_walk<-cumsum(rnorm(nsteps)) # Moving randomly on a 1D axis at each step according to SN
inter_step_diffs<-random_walk-dplyr::lag(random_walk,1)
inter_step_diffs[1]<-0
previous_t<-dplyr::lag(random_walk,1)
autocorrelation_plot_frame<-data.frame(t=1:nsteps,
walk=random_walk,
diffs=inter_step_diffs,
prev_step=previous_t)
autocorrelation_theme<-theme(panel.background=element_blank(),
text=element_text(size=18))
linear_model<-lm(data=autocorrelation_plot_frame,
walk~prev_step)
summary(linear_model)
residual_model<-lm(data=autocorrelation_plot_frame,
diffs~t)
summary(residual_model)
ggplot(autocorrelation_plot_frame,aes(x=t,y=walk))+
geom_point()+
autocorrelation_theme+
xlab('t')+
ylab('X(t)')+
ggtitle('Random Walk')
ggplot(autocorrelation_plot_frame,aes(x=prev_step,y=walk))+
geom_point()+
autocorrelation_theme+
xlab('X(t-1)')+
ylab('X(t)')+
ggtitle('Autocorrelation of Random Walk (R^2=97.6%)')
ggplot(autocorrelation_plot_frame,aes(x=t,y=diffs))+
geom_point()+
autocorrelation_theme+
ylab('X(t)-X(t-1)')+
xlab('t')+
ggtitle('Residuals of Random Walk (R^2~0)')
# Now we have
# lhs=a*rhs_a+b*rhs_b OR
# a*((rhs_a)-lhs)/rhs_b=-b
# Which means we can calculate an a for any b that will intersect both of these points
# Example
a<-seq(0.1,1,by=0.1)
b<--1*(rhs_a*a)-lhs
# We also could estimate different values of m and b, but we can quantify the error
# for each of those estimates given the observed data and this estimate will
# always have the lowest (error=0). Now, let's say we suspected our data
# actually have a quadratic relationship, and we wanted to estimate that with only these
# two points. We could perhaps say:
# Our model has 3 parameters
# y=ax^2+bx+c
# We could fit our data using the same approach as the linear model:
# y2-y1=a(x2^2-x1^2)+b(x2-x1)
# Then we get:
lhs<-obs_set_y[2]-obs_set_y[1]
rhs_b<-obs_set_x[2]-obs_set_x[1]
rhs_b<-obs_set_x[2]-obs_set_x[1]
# Now we have
# lhs=a*rhs_a+b*rhs_b OR
# a*((rhs_a)-lhs)/rhs_b=-b
# Which means we can calculate an a for any b that will intersect both of these points
# Example
a<-seq(0.1,1,by=0.1)
b<--1*(rhs_a*a)-lhs
a
b
# And for c we just need to plug in one of the observed points
c<-obs_set_y[1]-a*obs_set_x[1]^2-b*obs_set_x[1]
a
b
c
line_plotting_data<-seq(obs_set_x[1],obs_set_x[2],by=0.01)
possible_quadratics<-sapply(line_plotting_data,function(x) a*x^2+b*x+c)
View(possible_quadratics)
obs_set_x[1]
obs_set_x[2]
obs_set_y[1]
obs_set_y[2]
a
b
c
possible_quadratics<-sapply(line_plotting_data,function(x) (a*x^2)+(b*x)+c)
View(possible_quadratics)
line_plotting_data
possible_quadratics<-sapply(line_plotting_data,function(x) (a[1]*x^2)+(b[1]*x)+c[1])
possible_quadratics
lie_
line_plotting_data
a[1]*line_plotting_data[2]^2+b[1]*line_plotting_data[2]+c[1]
c
c[1]
b[1]
a[1]
b<--1*(rhs_a*a)+lhs
# And for c we just need to plug in one of the observed points
c<-obs_set_y[1]-a*obs_set_x[1]^2-b*obs_set_x[1]
line_plotting_data<-seq(obs_set_x[1],obs_set_x[2],by=0.01)
possible_quadratics<-sapply(line_plotting_data,function(x) (a[1]*x^2)+(b[1]*x)+c[1])
possible_quadratics
possible_quadratics<-sapply(line_plotting_data,function(x) (a*x^2)+(b*x)+c)
View(possible_quadratics)
plot(obs_set_x,obs_set_y)
for(i in 1:nrow(possible_quadratics)){
lines(line_plotting_data,possible_quadratics[i,])
}
plot(obs_set_x,obs_set_y)
for(i in 1:nrow(possible_quadratics)){
lines(line_plotting_data,possible_quadratics[i,])
}
abline(line_plotting_data,possible_quadratics[i,])
plot(obs_set_x,obs_set_y)
for(i in 1:nrow(possible_quadratics)){
abline(line_plotting_data,possible_quadratics[i,])
}
plot(obs_set_x,obs_set_y)+
for(i in 1:nrow(possible_quadratics)){
abline(line_plotting_data,possible_quadratics[i,])
}
dim(possible_quadratics)
possible_quadratics[1,]
plot(obs_set_x,obs_set_y)+
for(i in 1:nrow(possible_quadratics)){
lines(line_plotting_data,possible_quadratics[i,])
}
# Now we have
# lhs=a*rhs_a+b*rhs_b OR
# a*((rhs_a)-lhs)/rhs_b=-b
# Which means we can calculate an a for any b that will intersect both of these points
# Example
a<-seq(0.1,2,by=0.25)
b<--1*(rhs_a*a)+lhs
# And for c we just need to plug in one of the observed points
c<-obs_set_y[1]-a*obs_set_x[1]^2-b*obs_set_x[1]
line_plotting_data<-seq(obs_set_x[1],obs_set_x[2],by=0.01)
possible_quadratics<-sapply(line_plotting_data,function(x) (a*x^2)+(b*x)+c)
plot(obs_set_x,obs_set_y)+
for(i in 1:nrow(possible_quadratics)){
lines(line_plotting_data,possible_quadratics[i,])
}
random_walk2<-cumsum(rnorm(nsteps))
plot(random_walk,random_walk2)
plot_ly(pcoa_rna_frame,x=~Comp.1,y=~Comp.2,z=~Comp.3,color=~time_of_day)%>%
layout(title='PCoA of 18S RNA Diel Data',
scene=list(xaxis=list(title=paste0('PC1 ',eigenvalues[1],'%'),
scale=pcoa_rna$sdev[1]),
yaxis=list(title=paste0('PC2 ',eigenvalues[2],'%'),
scale=pcoa_rna$sdev[2]),
zaxis=list(title=paste0('PC3 ',eigenvalues[3],'%'),
scale=pcoa_rna$sdev[3])))
lines(obs_set_x,obs_set_x*m_estimate+b_estimate)
plot(obs_set_x,obs_set_y)
lines(obs_set_x,obs_set_x*m_estimate+b_estimate)
plot(obs_set_x,obs_set_y)+
lines(obs_set_x,obs_set_x*m_estimate+b_estimate)
plot(obs_set_x,obs_set_y,
xlab='X',ylab='Y',main='Linear Fit')+
lines(obs_set_x,obs_set_x*m_estimate+b_estimate)
plot(obs_set_x,obs_set_y,xlab='X',ylab='Y',main='Quadratic Fit')+
for(i in 1:nrow(possible_quadratics)){
lines(line_plotting_data,possible_quadratics[i,])
}
?rnorm
random_walks<-apply(rnorm(c(4,100)),2,cumsum)
rnorm(c(4,100))
random_walks<-apply(rnorm(4,100),2,cumsum)
rnorm(4,100)
random_walks<-apply(matrix(data=rnorm(4*100),ncol=100),2,cumsum)
View(random_walks)
plot(1:100,random_walks[1,])
random_walks<-apply(matrix(data=rnorm(4*100),ncol=4),2,cumsum)
View(random_walks)
plot(random_walks,random_walks)
pairs(random_walks,random_walks)
random_walks<-apply(matrix(data=rnorm(8*100),ncol=4),2,cumsum)
pairs(random_walks,random_walks)
random_walks<-apply(matrix(data=rnorm(8*100),ncol=8),2,cumsum)
pairs(random_walks,random_walks)
corrplot::corrplot(random_walks)
cor(random_walks)
cor.test(random_walks,method="pearson",alternative="two.sided")
cor.test(random_walks,random_walksmethod="pearson",alternative="two.sided")
cor.test(random_walks,random_walks,method="pearson",alternative="two.sided")
corrgram(cor(random_walks))
corrplot:corrplot(cor(random_walks))
corrplot::corrplot(cor(random_walks))
## Now it seems intuitive we wouldn't attempt to compare a time series with itself as a means of assessing correlation. However, this principle holds for all independent random walks, though to a smaller degree. Consider this
set.seed(89134)
random_walks<-apply(matrix(data=rnorm(8*100),ncol=8),2,cumsum)
random_corrs<-cor(random_walks)
pairs(random_walks,random_walks)
corrplot::corrplot(random_corrs)
## While not every single walk is super correlated, this sample of 8 already shows a handful of very strong correlations!
## Meanwhile if we did the differences
step_diffs<-apply(random_walks,2,function(x) x-dplyr::lag(x,1))
View(step_diffs)
??vst
meow<-DESeq2::makeExampleDESeqDataSet(n=2000,m=20)
View(meow@colData)
View(rna_counts)
dim(rna_counts)
## We want to start by maximally shaving down to temporal dynamics.
## The end result we want has a few characteristics: (1) the data for different biological units are intercomparable (ie, the means are similar) (2) we are protected from overinterpreting linear trends across our time series which may be due to autocorrelation (this is particularly important for attempting to analyze periodicity, where linear trends may be inconsistent)
## Here's how we're gonna get there: First, we're going to use the count data. This seems counterintuitive because we already said we want biological units to be intercomparable. The reason we use the counts is because often microbiome data is overdispersed, meaning that the variance of sequences with high counts and sequences with low counts between samples is not the same. If we want common sequences and rare sequences to be intercomparable, we therefore must account for this. A standard transformation is implemented in DESeq2 in R, though other variance-stabilizing transformation tools are also available.
# We won't subsample down for estimating variances because we don't have a huge number of sequences
transformed_counts<-DESeq2::vst(t(rna_counts),nsub=ncol(rna_counts))
## We want to start by maximally shaving down to temporal dynamics.
## The end result we want has a few characteristics: (1) the data for different biological units are intercomparable (ie, the means are similar) (2) we are protected from overinterpreting linear trends across our time series which may be due to autocorrelation (this is particularly important for attempting to analyze periodicity, where linear trends may be inconsistent)
## Here's how we're gonna get there: First, we're going to use the count data. This seems counterintuitive because we already said we want biological units to be intercomparable. The reason we use the counts is because often microbiome data is overdispersed, meaning that the variance of sequences with high counts and sequences with low counts between samples is not the same. If we want common sequences and rare sequences to be intercomparable, we therefore must account for this. A standard transformation is implemented in DESeq2 in R, though other variance-stabilizing transformation tools are also available.
# We won't subsample down for estimating variances because we don't have a huge number of sequences
transformed_counts<-DESeq2::varianceStabilizingTransformation(t(rna_counts))
View(transformed_counts)
apply(transformed_counts,1,function(x) length(x<0))
apply(transformed_counts,2,function(x) length(x<0))
dim(transformed_counts)
apply(transformed_counts,2,function(x) length(which(x<0)))
dim(rna_counts)
## We want to start by maximally shaving down to temporal dynamics.
## The end result we want has a few characteristics: (1) the data for different biological units are intercomparable (ie, the means are similar) (2) we are protected from overinterpreting linear trends across our time series which may be due to autocorrelation (this is particularly important for attempting to analyze periodicity, where linear trends may be inconsistent)
## Here's how we're gonna get there: First, we're going to use the count data. This seems counterintuitive because we already said we want biological units to be intercomparable. The reason we use the counts is because often microbiome data is overdispersed, meaning that the variance of sequences with high counts and sequences with low counts between samples is not the same. If we want common sequences and rare sequences to be intercomparable, we therefore must account for this. A standard transformation is implemented in DESeq2 in R, though other variance-stabilizing transformation tools are also available.
# First get distribution of how often each OTU occurs
n_counts<-colSums(rna_counts)
hist(n_counts)
hist(n_counts,bins=500)
hist(n_counts,breaks=500)
hist(log10(n_counts),breaks=500)
hist(log10(n_counts))
# This is what we mean by zero-inflated data.
length(which(n_counts)==0)
# This is what we mean by zero-inflated data.
length(which(n_counts==0))
dim(rna_counts)
rowSums(rna_counts)
meow<-colSums(rna_base)
summary(meow)
length(which(meow==0))
# This is histogram is diagnostic of zero-inflated data, a bunch of these sequences appear 0 times.
# Before we move on, because those sequences have very uninteresting temporal dynamics (at least not that we can observe), we will remove those.
rna_no_zero_seqs<-rna_counts[,-which(n_counts==0)]
# This is histogram is diagnostic of zero-inflated data, a bunch of these sequences appear 0 times.
# Before we move on, because those sequences have very uninteresting temporal dynamics (at least not that we can observe), we will remove those.
rna_no_zero_seqs<-rna_counts[,-which(n_counts==0)]
# We won't subsample down for estimating variances because we don't have a huge number of sequences
transformed_counts<-DESeq2::varianceStabilizingTransformation(t(rna_no_zero_seqs))
View(transformed_counts)
dim(transformed_counts)
??detrend
?scale
# Now we rescale so numbers are again intercomparable and now overdispersion has been
# dealt with (depending on how fancy you want to get and if you have any good theoretical statistics friends variance can be more completely modeled instead of normalized out and then you can do more stuff about hypothesis tests + include better error bars, but for the purpose of exploratory statistics this'll do as far as authors are concerned)
trans_dt_scaled<-apply(transformed_detrended,1,scale)
## Whatever we may make some additional choices but plugging ahead
# Now we remove linear trends from the time series to discourage distances being functions
# of anything other than temporal form
transformed_detrended<-apply(transformed_counts,1,pracma::detrend)
# Now we rescale so numbers are again intercomparable and now overdispersion has been
# dealt with (depending on how fancy you want to get and if you have any good theoretical statistics friends variance can be more completely modeled instead of normalized out and then you can do more stuff about hypothesis tests + include better error bars, but for the purpose of exploratory statistics this'll do as far as authors are concerned)
trans_dt_scaled<-apply(transformed_detrended,1,scale)
View(transformed_counts)
View(transformed_detrended)
# Now we rescale so numbers are again intercomparable and now overdispersion has been
# dealt with (depending on how fancy you want to get and if you have any good theoretical statistics friends variance can be more completely modeled instead of normalized out and then you can do more stuff about hypothesis tests + include better error bars, but for the purpose of exploratory statistics this'll do as far as authors are concerned)
trans_dt_scaled<-apply(transformed_detrended,2,scale)
View(trans_dt_scaled)
View(transformed_counts)
## We lost rownames so tack those back on
dim(trans_dt_scaled)
dim(transformed_cout)
dim(transformed_count)
dim(transformed_counts)
## We lost rownames so tack those back on
rownames(trans_dt_scaled)<-colnames(transformed_counts)
View(trans_dt_scaled)
# Now we're ready to generate a distance matrix for clustering
# Which begs the question? What distance to use? Because these are z-score transformed, we
# have the luxury of using a magnitude-sensitive distance metric like euclidean distance
# Specifically for time series as well, the short time series distance has been developed which also includes a time step size regularization to account for uneven sampling, but for regular sampling intervals throughout the entire time series is similar to euclidean distance. We'll demonstrate euclidean distance, but recommend reading up on distance metrics because the best metrics will vary by experimental design!
temporal_dmat<-dist(trans_dt_scaled)
# Now we're ready to generate a distance matrix for clustering
# Which begs the question? What distance to use? Because these are z-score transformed, we
# have the luxury of using a magnitude-sensitive distance metric like euclidean distance
# Specifically for time series as well, the short time series distance has been developed which also includes a time step size regularization to account for uneven sampling, but for regular sampling intervals throughout the entire time series is similar to euclidean distance. We'll demonstrate euclidean distance, but recommend reading up on distance metrics because the best metrics will vary by experimental design!
temporal_dmat<-dist(t(trans_dt_scaled))
## (Fun side note: this author has not done this analysis on these data before, so we will learn together!)
# Let's try 2 common clustering methods and a couple hipper ones
# First, the classic hierarchical clustering
hc_full_cluster<-hclust(temporal_dmat)
plot(hc_full_cluster)
# This method is a little different from other clustering methods because it is "complete", in that it continues to cluster by dividing the data into halves with the least inter-half similarity until you go all the way back to all sequences being their own cluster. Therefore, we run the full algorithm, then extract the clusterings for n # clusters to analyze and validate
hc_clusts<-sapply(n_clusts,function(x) hcut(hc_full_cluster,x))
# No we have the pairwise time-series distances between each sequence unit. Now we can try some different cluster methods and see what happens.
# Because we are doing hard clustering, we must tell the computer how many clusters we want for these methods. This sounds not very robust, but we'll show how you can find a parsimonious clustering by comparing multiple clusterings at different # clusters
n_clusts<-2:10 # Trying for a variety of cluster numbers
# This method is a little different from other clustering methods because it is "complete", in that it continues to cluster by dividing the data into halves with the least inter-half similarity until you go all the way back to all sequences being their own cluster. Therefore, we run the full algorithm, then extract the clusterings for n # clusters to analyze and validate
hc_clusts<-sapply(n_clusts,function(x) hcut(hc_full_cluster,x))
??hcut
?hclust
# This method is a little different from other clustering methods because it is "complete", in that it continues to cluster by dividing the data into halves with the least inter-half similarity until you go all the way back to all sequences being their own cluster. Therefore, we run the full algorithm, then extract the clusterings for n # clusters to analyze and validate
hc_clusts<-sapply(n_clusts,function(x) cutree(hc_full_cluster,x))
# This method is a little different from other clustering methods because it is "complete", in that it continues to cluster by dividing the data into halves with the least inter-half similarity until you go all the way back to all sequences being their own cluster. Therefore, we run the full algorithm, then extract the clusterings for n # clusters to analyze and validate
hc_clusts<-lapply(n_clusts,function(x) cutree(hc_full_cluster,x))
?pamkr
?pamk
??pamk
kmed_clusts<-lapply(n_clusts, function(x) cluster::pam(temporal_dmat,k=x))
??hopkins
dim(trans_dt_scaled)
## First we calculate hopkin's statistic before proceeding (note: the limitation of this index on null distance distributions makes it kind of a weak argument, but it is nice just to check)
h_index<-clustertend::hopkins(trans_dt_scaled,n=ncol(trans_dt_scaled)-1)
## First we calculate hopkin's statistic before proceeding (note: the limitation of this index on null distance distributions makes it kind of a weak argument, but it is nice just to check)
h_index<-clustertend::hopkins(trans_dt_scaled,n=nrow(trans_dt_scaled)-1)
h_index
x<-matrix(runif(200,1,100),50,4)
dim(x)
hopkins(x,n=10)
clustertend::hopkins(x,n=10)
dim(trans_dt_scaled)
## First we calculate hopkin's statistic before proceeding (note: the limitation of this index on null distance distributions makes it kind of a weak argument, but it is nice just to check)
h_index<-clustertend::hopkins(t(trans_dt_scaled),n=1000)
## First we calculate hopkin's statistic before proceeding (note: the limitation of this index on null distance distributions makes it kind of a weak argument, but it is nice just to check)
h_index<-clustertend::hopkins(t(trans_dt_scaled),n=100)
h_index
??clustertend
??calinhara
?calinhara
meow<-hc_clusts[[1]]
## Now we can look at C-H statistic (I'll look up the name at some point)
hc_stats<-lapply(hc_clusts,function(x) fpc::cluster.stats(temporal_dmat,
clustering=x))
meow<-kmed_clusts[[1]]
meow$clustering
kmed_stats<-lapply(hc_clusts, function(x) fpc::cluster.stats(temporal_dmat,
clustering=x$clustering))
kmed_stats<-lapply(kmed_clusts, function(x) fpc::cluster.stats(temporal_dmat,
clustering=x$clustering))
meow<-hc_stats[[1]]
meow$within.cluster.ss
hc_wss<-ripping_stats(hc_stats,function(x) x$within.cluster.ss)
## Now we want to actually compare clusterings, we can do this with several indices
## First and possibly most familar, error.
## We write a helper function to be less redundant
ripping_stats<-function(list,func){
## Essentially all this function does is implements a function (func) on a list
## and coerces the output to a column vector (this will be handy when we want to make a data frame)
output<-do.call(rbind(lapply(list,func)))
return(output)
}
hc_wss<-ripping_stats(hc_stats,function(x) x$within.cluster.ss)
## Now we want to actually compare clusterings, we can do this with several indices
## First and possibly most familar, error.
## We write a helper function to be less redundant
ripping_stats<-function(list,func){
## Essentially all this function does is implements a function (func) on a list
## and coerces the output to a column vector (this will be handy when we want to make a data frame)
output<-do.call(rbind,lapply(list,func))
return(output)
}
hc_wss<-ripping_stats(hc_stats,function(x) x$within.cluster.ss)
hc_wss
kmed_wss<-ripping_stats(kmed_stats,function(x) x$within.cluster.ss)
wss_frame<-data.frame(nc=n_clusts,hc=hc_wss,kmed=kmed_wss)
wss_frame %>%
ggplot(aes(x=nc)) %>%
geom_point(aes(y=hc))
wss_frame %>%
ggplot(aes(x=nc)) +
geom_point(aes(y=hc))
stats_list<-rep(list(hc_stats,kmed_stats),each=4)
length(stats_list)
func_list<-rep(list(function(x) x$cluster.number,
function(x) x$within.cluster.ss,
function(x) x$avg.silwidth,
function(x) x$ch),2)
length(func_list)
??map2
func_list<-rep(list(function(x) x$cluster.number,
function(x) x$within.cluster.ss,
function(x) x$avg.silwidth,
function(x) x$ch),2)
stats_list<-rep(list(hc_stats,kmed_stats),each=4)
meow<-purrr::map2(stats_list,func_list,ripping_stats)
meow
method<-rep(c('hc','kmed'),each=length(n_clusts)*length(collected_stats))
collected_stats<-purrr::map2(stats_list,func_list,ripping_stats)
nclusts<-rep(n_clusts,length(collected_stats))
method<-rep(c('hc','kmed'),each=length(n_clusts)*length(collected_stats))
meow<-do.call(rbind,collected_stats)
nclusts<-rep(n_clusts,length(collected_stats))
method<-rep(c('hc','kmed'),each=length(n_clusts)*length(collected_stats)/2)
index_frame<-data.frame(index=collected_stats,
nc=nclusts,
method=method)
ind_name<-rep(c('n','ss','sil','ch'),each=length(n_clusts)) %>%
rep(2)
ind_name
collected_stats[1:10]
index_frame<-data.frame(index=do.call(rbind,collected_stats),
nc=nclusts,
method=method,
ind=ind_name)
index_frame %>%
filter(ind=='ss') %>%
ggplot(aes(x=nclusts,y=index,col=method)) +
geom_point() +
geom_line(aes(group=method))
stats_list<-rep(list(hc_stats,kmed_stats),each=4)
ind_name<-rep(c('n','ss','sil','ch'),each=length(n_clusts)) %>%
rep(2)
index_frame<-data.frame(index=do.call(rbind,collected_stats),
nc=nclusts,
method=method,
ind=ind_name)
index_frame %>%
filter(ind=='ss') %>%
ggplot(aes(x=nclusts,y=index,col=method)) +
geom_point() +
geom_line(aes(group=method))
index_frame %>%
filter(ind=='ss') %>%
ggplot(aes(x=nc,y=index,col=method)) +
geom_point() +
geom_line(aes(group=method))
index_frame %>%
filter(ind=='ch') %>%
ggplot(aes(x=nc,y=index,col=method)) +
geom_point() +
geom_line(aes(group=method)) +
ylab('Within Cluster Sum Square Error') +
xlab('Number Clusters')
dim(rna_no_zero_seqs)
# We can back and forth on this -- I like doing the VSN as opposed to going straight to CLR on relative abundances because we can tackle heteroskedasticity in the data more head-on, but also we can demonstrate before we do this whether that's something to worry about or not by like, plotting within-otu SD before against within-otu mean
within_seq_means<-apply(rna_no_zero_seqs,2,mean)
within_seq_vars<-apply(rna_no_zero_seqs,2,var)
plot(within_seq_means,within_seq_vars)
plot(within_seq_means,within_seq_vars,log='xy')
plot(within_seq_means,within_seq_vars,log='x')
plot(within_seq_means,within_seq_vars,log='xy')
dim(transformed_counts)
within_trans_means<-apply(transformed_counts,1,mean)
within_trans_vars<-apply(transformed_counts,1,var)
plot(within_trans_means,within_trans_vars)
